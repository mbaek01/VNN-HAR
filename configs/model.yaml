baseline:
  num_layers: 4
  num_units: 128
  activation_fn: leaky_relu

baseline_attn:
  nb_units: 128
  activation_fn: relu

vn_baseline_attn:
  nb_units: 120 # 120 // 3 has to be multiple sof num_head(=4) in attn 
  activation_fn: vn_leaky_relu
  
vnn_mlp:
  num_layers: 5
  num_units: 128

sa_har:
  nb_units: 128

deepconvlstm:
  nb_conv_blocks: 2
  nb_filters: 64
  # dilation: 1
  batch_norm: 0
  filter_width: 5
  nb_layers_lstm: 2
  drop_prob: 0.5
  nb_units_lstm: 128

deepconvlstm_attn:
  nb_conv_blocks: 2
  nb_filters: 64
  # dilation: 1
  batch_norm: 0
  filter_width: 5
  nb_layers_lstm: 2
  drop_prob: 0.5
  nb_units_lstm: 128