baseline:
  num_layers: 4
  num_units: 128
  activation_fn: leaky_relu

baseline_attn:
  nb_units: 128
  activation_fn: relu
  
vnn_mlp:
  num_layers: 5
  num_units: 128

sa_har:
  nb_units: 128

deepconvlstm_attn:
  nb_conv_blocks: 2
  nb_filters: 64
  # dilation: 1
  batch_norm: 0
  filter_width: 5
  nb_layers_lstm: 2
  drop_prob: 0.5
  nb_units_lstm: 128